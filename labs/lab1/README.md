# Lab 1

The goal of this lab is for you to set up Amazon Web Services ("Amazon
Cloud") and gain experiment with working with data in various degrees
of structure.

You will the a dataset of Tweeets encoded in multiple ways to compute
some summary information and reflect on the pros and cons of each
approach.


# Step 0: Setup Amazon EC2

Many of the labs in this class will use Amazon's cloud computing infrastructure.
Using a cloud service like Amazon makes it easy to share data sets, and quickly run any number virtual machines that 
are identical for all students in the class.
We have credits from Amazon, which we will use for later labs (in this lab, we will use a free "micro" instance.)

### Sign up and setup the OS

**Signup**: [register for an account](https://aws-portal.amazon.com/gp/aws/developer/registration/index.html)

You will need to provide a credit card, however the class has Amazon credits so
you should not expect to _use_ the credit card.  Once the class registration has 
settled down, we will add you to the class's Amazon groups.

**Launch an instance**

1. Go to [http://aws.amazon.com](http://aws.amazon.com) and click 'AWS Management Console' under 'My Account/Console' 
in the upper right.  
1. Click EC2
1. Click Launch Instance.  
1. Use the "Classic Wizard". As of this writing the "Quick Launch Wizard" would not successfully launch.
1. Select one of the Ubuntu Server images ("AMIs"); it doesn't matter exactly which one for this lab.
1. Specify 1 instance of type "t1.micro". Amazon lets you launch one micro-instance for free, so this won't cost you anything to launch.  
1. You don't care about the subnet, and can simply click "Continue" on the "Advanced Instance Options", "Storage Device Configuration", and "Add Tags" pages.
1. You will need to specify a key value pair, or create a new one.  If you choose to create a new one, make sure you download it and save it (your should have a .pem file).
1. The default security group is fine.
1. Click "Launch".  It will take a few minutes for the instance to launch.
1. After the instances launches, click on it to obtain its DNS name.  It should look something like ec2-xx-xxx-xx-xxx.us-west-2.compute.amazonaws.com

**SSH to Your Instance**: 

Type something like:

ssh -i <PEM FILE> ubuntu@<public address>

Where <PEM FILE> is key file you downloaded when launching the instance.

**Setup the OS**: ensure the following packages are available using the Ubuntu package management tool apt-get.  

To install a package, type:

sudo apt-get install <packagename>

Make sure you have the following packages:

* python2.7
  * python-psycopg2
  * python-sqlalchemy
* python-protobuf
* postgresql
* sqlite3
* git

Checkout the class repository

    git clone https://github.com/mitdbg/asciiclass.git


xxx this gives me the error:
Warning: Permanently added 'github.com,192.30.252.130' (RSA) to the list of known hosts.
Permission denied (publickey).
fatal: The remote end hung up unexpectedly


Go to lab 1:

    cd asciiclass/labs/lab1


# Step 1: Analysis on JSON

### Setup

In this step, you will write scripts in your favorite language to analyze the json dataset.  

First decompress the data file:

    gzip -d twitter.json.gz

`twitter.json` contains a json-encoded tweet on each line.  Check out twitter's documentation for information about the tweets or simply read the file.

### Analyses

Perform the following analyses and report the results:

1. The number of delete messages in the dataset
2. The number of tweets that have a place attribute
2. The number of tweets that are replies to any other tweet
3. The number of tweets that are replies to another tweet in this dataset
4. The top five uids that have tweeted the most.
3. The names of the top five places by number of tweets.  (Tweets may have a "place" attribute that describes where the tweet is from)

### Questions

1. Briefly describe your approach for performing these analyses.
1. Briefly reflect on the difficulty of performing these analyses in terms of time taken, code redundancy, or anything else of interest.

# Step 2: Analyses on Protocol Buffers

### Setup

In this step, you will use protocol buffers to perform similar analyses.  The definitions are found in `twitter.proto` and the data files are in `twitter.pb`.  

We generated the protocol buffer's python bindings by running:

    protoc twitter.proto --python_out=.
    
This should generate a `twitter_pb2.py` that you can `import` in your python scripts.

`twitter.pb` is generated by calling `python encode.py`.

For details about protocol buffers, [read the reference](https://developers.google.com/protocol-buffers/docs/reference/overview).


### Analyses

Perform the same analyses as step 1 using the dataset encoded as protocol buffers.



### Questions

1. What were the main reasons that made the analyses easier than using
   the JSON dataset?
2. What may be a case where using the JSON dataset would be easier than protocol buffers?

# Step 3: Analyses on database records

### Setup

In this step, you will be working with the twitter data encoded as a
sqlite3 database file.  The file, `twitter.db` was generated using `python createdb.py`.
The schemas are defined in `twitter.ddl`.

Start a sqlite3 prompt by typing:

    sqlite3 twitter.db

For SQL help, refer to the fantastic [sqlite documentation](http://www.sqlite.org/docs.html)
and [postgresql's documentation](http://www.postgresql.org/docs/).

### Analyses

Now perform the same analyses as step 1

### Questions

1. How did these analyses compare with step 1 and 2?
2. Read the schema and protocol buffer definition files.  What strikes you as some main
   differences between the two?  Similarities?
3. What may be some querios that would be easier with protocol buffers or JSON encoded files?
4. In what cases does importing the data into a database make sense?

