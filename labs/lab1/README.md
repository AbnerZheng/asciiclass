# Lab 1

The goal of this lab is for you to set up  amazon/cloud  experience the pain of working
with data in various degrees of structure.  

You will the same tweet dataset encoded in multiple ways to compute
some summary information and reflect on the pros and cons of each
encoding.  


# Step 0: Setup Amazon EC2

Many of the labs in this class will involve running on cloud computing infrastructure.
so we ask that you to run the labs an such an infrastructure from the very beginning.  
The class has recieved Amazon credits so the instructions will be written towards their
services.

If you would like to use another cloud infrastructure, feel free
to do so but you're on your own!

### Sign up and setup the OS

**Signup**: [register for an account](https://aws-portal.amazon.com/gp/aws/developer/registration/index.html)

You will need to provide a credit card, however the class has amazon credits so
you should not expect to _use_ the credit card.  Once the class registration has 
settled down, we will add you to the class's amazon groups.

**Launch an instance**

1. Go to [http://aws.amazon.com](http://aws.amazon.com) and click 'AWS Management Console' under 'My Account/Console' 
in the upper right.  
1. Click EC2
1. Click Launch Instance.  Amazon lets you launch one micro-instance for free, so we strongly recommend 
   that you do this.
1. Use one of the Ubuntu Server images (doesn't matter exactly which one for this lab)
1. Check its public address.  It should look something like ec2-xx-xxx-xx-xxx.us-west-2.compute.amazonaws.com

**Download your keys**: you need to download a pem file in order to ssh to your instance.
Download it and ssh:

    ssh -i <PEM FILE> ubuntu@<public address>

**Setup the OS**: ensure the following packages are available using apt-get.  

* python 2.7
  * psycopg2
  * sqlalchemy
* protocol buffers
* postgresql
* sqlite3
* git

Checkout the class repository

    git clone git@github.com:sirrice/asciiclass.git

Go to lab 1:

    cd asciiclass/labs/lab1


# Step 1: Analyses on JSON

### Setup

In this step, you will write scripts in your favorite language to analyze the json dataset.  

First decompress the data file:

    gzip -d twitter.json.gz

`twitter.json` contains a json-encoded tweet on each line.  Check out twitter's documentation for information about the tweets or simply read the file.

### Analyses

Perform the following analyses and report the results:

1. The number of delete messages in the dataset
2. The number of tweets that have a place attribute
2. The number of tweets that are replies to any other tweet
3. The number of tweets that are replies to another tweet in this dataset
4. The top five uids that have tweeted the most.
3. The names of the top five places by number of tweets.  (Tweets may have a "place" attribute that describes where the tweet is from)

### Questions

1. Briefly describe your approach for performing these analyses.
1. Briefly reflect on the difficulty of performing these analyses in terms of time taken, code redundancy, or anything else of interest.

# Step 2: Analyses on Protocol Buffers

### Setup

In this step, you will use protocol buffers to perform similar analyses.  The definitions are found in `twitter.proto` and the data files are in `twitter.pb`.  

We generated the protocol buffer's python bindings by running:

    protoc twitter.proto --python_out=.
    
This should generate a `twitter_pb2.py` that you can `import` in your python scripts.

`twitter.pb` is generated by calling `python encode.py`.

For details about protocol buffers, [read the reference](https://developers.google.com/protocol-buffers/docs/reference/overview).


### Analyses

Perform the same analyses as step 1 using the dataset encoded as protocol buffers.



### Questions

1. What were the main reasons that made the analyses easier than using
   the JSON dataset?
2. What may be a case where using the JSON dataset would be easier than protocol buffers?

# Step 3: Analyses on database records

### Setup

In this step, you will be working with the twitter data encoded as a
sqlite3 database file.  The file, `twitter.db` was generate using `python createdb.py`.
The schemas are defined in `twitter.ddl`.

Start a sqlite3 prompt by typing:

    sqlite3 twitter.db

For SQL help, refer to the fantastic [sqlite documentation](http://www.sqlite.org/docs.html)
and [postgresql's documentation](http://www.postgresql.org/docs/).

### Analyses

Now perform the same analyses as step 1

### Questions

1. How did these analyses compare with step 1 and 2?
2. Read the schema and protocol buffer definition files.  What strikes you as some main
   differences between the two?  Similarities?
3. What may be some querios that would be easier with protocol buffers or JSON encoded files?
4. In what cases does importing the data into a database make sense?

