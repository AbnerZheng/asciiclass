
# WARNING: THIS LAB IS NOT COMPLETE.  DO NOT START IT!


# Lab 2

The goal of this lab is for you to experiment with working with data
in various degrees of structure.

You will the a dataset of Tweeets encoded in multiple ways to compute
some summary information and reflect on the pros and cons of each
approach.

To start, go to the AWS console and start your EC2 instance.  SSH to it, and go to lab 2:

    cd asciiclass/labs/lab2

Update the contents of lab2:

    git pull

# Step 1: Analysis on JSON

### Setup

In this step, you will write scripts in your favorite language to analyze the json dataset.  

First decompress the data file:

    gzip -d twitter.json.gz

`twitter.json` contains a json-encoded tweet on each line.  Check out twitter's documentation for information about the tweets or simply read the file.

### Analyses

Perform the following analyses and report the results:

1. The number of delete messages in the dataset
2. The number of tweets that have a place attribute
2. The number of tweets that are replies to any other tweet
3. The number of tweets that are replies to another tweet in this dataset
4. The top five uids that have tweeted the most.
3. The names of the top five places by number of tweets.  (Tweets may have a "place" attribute that describes where the tweet is from)

### Questions

1. Briefly describe your approach for performing these analyses.
1. Briefly reflect on the difficulty of performing these analyses in terms of time taken, code redundancy, or anything else of interest.

# Step 2: Analyses on Protocol Buffers

### Setup

In this step, you will use protocol buffers to perform similar analyses.  The definitions are found in `twitter.proto` and the data files are in `twitter.pb`.  

We generated the protocol buffer's python bindings by running:

    protoc twitter.proto --python_out=.
    
This should generate a `twitter_pb2.py` that you can `import` in your python scripts.

`twitter.pb` is generated by calling `python encode.py`.

For details about protocol buffers, [read the reference](https://developers.google.com/protocol-buffers/docs/reference/overview).


### Analyses

Perform the same analyses as step 1 using the dataset encoded as protocol buffers.

### Questions

1. What were the main reasons that made the analyses easier than using
   the JSON dataset?
2. What may be a case where using the JSON dataset would be easier than protocol buffers?

# Step 3: Analyses on database records

### Setup

In this step, you will be working with the twitter data encoded as a
sqlite3 database file.  The file, `twitter.db` was generated using `python createdb.py`.
The schemas are defined in `twitter.ddl`.

Start a sqlite3 prompt by typing:

    sqlite3 twitter.db

For SQL help, refer to the fantastic [sqlite documentation](http://www.sqlite.org/docs.html)
and [postgresql's documentation](http://www.postgresql.org/docs/).

### Analyses

Now perform the same analyses as in step 1, but in the sqlite-encoded data.

### Questions

1. How did these analyses compare with step 1 and 2?
2. Read the schema and protocol buffer definition files.  What strikes you as some main
   differences between the two?  Similarities?
3. What may be some queries that would be easier with protocol buffers or JSON encoded files?
4. In what cases does importing the data into a database make sense?

# Step 4: Analyses in MongoDB

### Setup

In this step, we will import and query the json data we've collected
in MongoDB.  First, let's get the data into Mongo:

    mongoimport -d lab2 -c tweets twitter.json

This will, in the database `lab2`, create a collection called `tweets`
from the JSON blobs inside `twitter.json`.

To access the `lab2` database, type

   mongo lab2

Refer to Mongo's detailed [query language documentation](http://docs.mongodb.org/manual/reference/method/db.collection.find/#db.collection.find) for help.

### Analyses

Now perform the same analyses as in step 1, but on the Mongo-encoded data.

# Step 5: Reflection

1. In terms of lines of code, when did various approaches shine?  Think about the challenges of defining schemas, loading and storing the data, and running queries.
2. What other measures can we use to compare these different approaches?  Which system is better by those measures?