# Lab 2

The goal of this lab is for you to experiment with working with data
in various degrees of structure.

You will the a dataset of Tweeets encoded in multiple ways to compute
some summary information and reflect on the pros and cons of each
approach.

To start, go to the AWS console and start your EC2 instance.  SSH into it.
First, you need to check out/update the files for lab2:

    cd asciiclass/labs/
    git pull

Then :

    cd lab2

# Step 1: Analysis on JSON

### Setup

In this step, you will write scripts in your favorite language to analyze the json dataset.  

First decompress the data file:

    gzip -d twitter.json.gz

`twitter.json` contains a json-encoded tweet on each line.  Check out (twitter's documentation)[https://dev.twitter.com/docs/platform-objects/tweets] for information about the tweets or simply read the file.

### Analyses

Perform the following analyses (using whatever programming langugae or environment you like) and report the results:

1. Find the number of deleted messages in the dataset
1. Find the number of tweets that are replies to another tweet in this dataset
1. Find the five uids that have tweeted the most.
1. Find the names of the top five places by number of tweets.  (Tweets may have a "place" attribute that describes where the tweet is from).

### Questions

1. Briefly describe your approach for performing these analyses.
1. Briefly reflect on the difficulty of performing these analyses in terms of time taken, code redundancy, or anything else of interest.

# Step 2: Analyses using Protocol Buffers

### Setup

In this step, you will use protocol buffers to perform similar analyses. 

For details about protocol buffers, [read this page](https://developers.google.com/protocol-buffers/docs/reference/overview).

The protocol buffer definitions are found in `twitter.proto` and the data files are in `twitter.pb`.  

We generated the protocol buffer's python bindings by running:

    protoc twitter.proto --python_out=.
    
This should generate a `twitter_pb2.py` that you can `import` in your python scripts.

`twitter.pb` is generated by calling `python encode.py`, but you do
not need to re-create the file.  Note that this script does not
include all fields: we serialized only the subset of fields that are
necessary for answering the questions.



### Analyses

Perform the same analyses as step 1 using the dataset encoded as protocol buffers.

### Questions

1. What were the main reasons that made the analyses easier than using
   the JSON dataset?
2. Can you describe a case where directly operating on the JSON dataset would be easier than protocol buffers?

# Step 3: Analyses on database records

### Setup

In this step, you will be working with the twitter data encoded as a
sqlite3 database file.  The file, `twitter.db` was generated using
`python createdb.py`, but you do not need to re-run this script.  The
schemas are defined in `twitter.ddl`. Note that this schema does not
include all fields: we includes only the subset of fields that are
necessary for answering the questions.

Start a sqlite3 prompt by typing:

    sqlite3 twitter.db

For SQL help, refer to the fantastic [sqlite documentation](http://www.sqlite.org/docs.html)
and [postgresql's documentation](http://www.postgresql.org/docs/).

### Analyses

Perform the same analyses as in step 1, but in the sqlite-encoded data.

### Questions

1. How did these analyses compare with step 1 and 2?
2. Read the schema and protocol buffer definition files.  What are the main
   differences between the two?  Are there any similarities?
3. Describe one question that would be easier to answer with protocol buffers or JSON encoded files than via a SQL query.
4. In what cases does importing the data into a database make sense?
5. What fields in the original JSON structure would be difficult to
   convert to relational database schemas?

# Step 4: Analyses in MongoDB

### Setup

In this step, we will import and query the json data we've collected
in MongoDB.  First, let's get the data into Mongo:

    mongoimport -d lab2 -c tweets twitter.json

This will, in the database `lab2`, create a collection called `tweets`
from the JSON blobs inside `twitter.json`.

To access the `lab2` database, type

    mongo lab2

Refer to Mongo's detailed [query language documentation](http://docs.mongodb.org/manual/reference/method/db.collection.find/#db.collection.find) for help.

### Analyses

Now perform the same analyses as in step 1, but on the Mongo-encoded data.

# Step 5: Reflection

1. In terms of lines of code, when did various approaches shine?  Think about the challenges of defining schemas, loading and storing the data, and running queries.
2. What other measures can we use to compare these different approaches?  Which system is better by those measures?

# Handing in your work

You should create a text file with your name, the results of the six
analyses, and your answers to the questions in Steps 1, 2, 3, 4, and
5.  Where possible, show the query you used (describe it at a high
level if the code/query takes up more than a few lines).  Upload it to
the [course Stellar
site](http://stellar.mit.edu/S/course/6/fa13/6.885/) as the "lab2"
assignment.